{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Develop a baseline model for comparing performance on models with more features\n",
    "- Encode categorical features so that model can make better use of the information\n",
    "- Generate new features to provide more information for the medel\n",
    "- Select features to reduce overfitting and increase prediction speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ks = pd.read_csv('../input/kickstarter-projects/ks-projects-201801.csv',\n",
    "                 parse_dates=['deadline', 'launched'])\n",
    "ks.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing target column**\n",
    "\n",
    "First I'll look at project states and convert the column into something we can use as targets in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(ks.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have six states, how many records of each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks.groupby('state')['ID'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop live projects\n",
    "ks = ks.query('state != \"live\"')\n",
    "\n",
    "#Add outcome column, \"successful\" == 1, others are 0\n",
    "ks = ks.assign(outcome= (ks['state'] == 'successful').astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting timestamps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = ks.assign(hour = ks.launched.dt.hour,\n",
    "              day = ks.launched.dt.day,\n",
    "              month = ks.launched.dt.month,\n",
    "              year = ks.launched.dt.year)\n",
    "ks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepping categorical variables**\n",
    "\n",
    "Now for the categorical variables -- `category`, `currency`, and `country` -- I'll need to convert them into integers so our model can use the data. For this I'll use scikit-learn's `LabelEncoder`.This assigns an integer to each value of the categorical feature and replaces those values with the integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "encode = LabelEncoder()\n",
    "\n",
    "#Apply the label encoder to each column\n",
    "encoded = ks[cat_features].apply(encoder.fit_transfrom)\n",
    "encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll collect all the features we'll use in a new dataframe and use that to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since ks and encode have the same index and I can easily join them\n",
    "data = ks[['goal', 'hour', 'day', 'month', 'year', 'outcome']].join(encoded)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating training, validation, and test splits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_fraction= 0.1\n",
    "valid_size = int(len(data) * valid_fraction)\n",
    "\n",
    "train = data[:-2 * valid_size]\n",
    "valid = data[-2 * valid_size: -valid_size]\n",
    "test = data[-valid_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general you want to be careful that each data set has the same proportion of target classes.I'll print out the fraction of successful outcomes for each of our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in [train, valid, test]:\n",
    "    print(f\"Outcome fraction = {each.outcome.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good, each set is around 35% true outcomes likely because the data was well randomized beforehand. A good way to do this automatically is with \n",
    "\n",
    "`sklearn.model_selection.StratifiedShuffleSplit` but I don't need to use it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training a LightGBM model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this course we'll be using a LightGBM model. This is a tree-based model that typically provides the best performance, even compared to XGBoost, It's also relatively fast to train. We won't do hyperparemeter optimization because that isn't the goal of this course. So, our models won't be the absolute best performance you can get. But you'll still see model performance improve as we do feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "feature_cols = train.columns.drop('outcome')\n",
    "\n",
    "dtrain = lgb.Dataset(train[feature_cols], label= train['outcome'])\n",
    "dvalid = lgb.Dataset(valid[feature_cols], label= valid['outcome'])\n",
    "\n",
    "param = {'num_leaves': 64, 'objective': 'binary'}\n",
    "param['metric'] = 'auc'\n",
    "num_round = 1000\n",
    "bst = lgb.train(param, dtrain, num_round, valid_sets= [dvalid], early_stopping_\n",
    "               rounds= 10, verbose_eval= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making predictions& evaluating the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's make predictions on the test set with the model and see how well it preforms.An importment thing to remember is that you can overfit to the validation data. This is way we need a testset that the model never sees until the final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "ypred = bst.predict(test[feature_cols])\n",
    "score = metrics.roc_auc_score(test['outcome'], ypred)\n",
    "\n",
    "print(f\"Test AUC score: {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
